{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsBGBjaBEgPl"
   },
   "source": [
    "# **How to Train YOLOv7 on a Custom Dataset**\n",
    "\n",
    "This tutorial is based on the [YOLOv7 repository](https://github.com/WongKinYiu/yolov7) by WongKinYiu. This notebook shows training on **your own custom objects**. Many thanks to WongKinYiu and AlexeyAB for putting this repository together.\n",
    "\n",
    "\n",
    "### **Accompanying Blog Post**\n",
    "\n",
    "We recommend that you follow along in this notebook while reading the blog post on [how to train YOLOv7](https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial/), concurrently.\n",
    "\n",
    "### **Steps Covered in this Tutorial**\n",
    "\n",
    "To train our detector we take the following steps:\n",
    "\n",
    "1. Install YOLOv7 dependencies\n",
    "2. Load custom dataset from Roboflow in YOLOv7 format\n",
    "3. Run YOLOv7 training\n",
    "4. Evaluate YOLOv7 performance\n",
    "5. Run YOLOv7 inference on Test Images\n",
    "6. Run YOLOv7 inference on Video\n",
    "7. Run YOLOv7 inference on Webcam\n",
    "\n",
    "\n",
    "### **Preparing a Custom Dataset**\n",
    "\n",
    "In this tutorial, we will utilize our own object detection dataset (Trash dataset) with some objects classified as trash while the other as not trash. **Follow the link for dataset** [Trash Dataset](https://roboflow.com/as-waste). Instructions on how to download dataset will be given subsequently.\n",
    "\n",
    "# **Want to Become a Computer Vision Expert?**\n",
    "üíª [ Get Started](https://augmentedstartups.info/YOLOv7GetStarted) with YOLOv7.  <br>\n",
    "‚≠ê Download other Projects at the [AI Vision Store](https://store.augmentedstartups.com)<br>\n",
    "‚òï Enjoyed this Tutorial? - Support me by Buying Me a [Chai/Coffee](https://bit.ly/BuymeaCoffeeAS)\n",
    "\n",
    "\n",
    "# **About**\n",
    "\n",
    "[Augmented Startups](https://www.augmentedstartups.com) provides tutorials in AI Computer Vision and Augmented Reality. With over **100K subscribers** on our channel, we teach state-of-art models and build apps and projects that solve real-world problems. \n",
    "\n",
    "\n",
    "![picture](https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/sites/104576/images/Vc8nhFV6Rgmi402Wqm0Q_AugmentedStartupsSideROBOTICSNEW.png)\n",
    "\n",
    "[Roboflow](https://roboflow.com/as) enables teams to deploy custom computer vision models quickly and accurately. Convert data from to annotation format, assess dataset health, preprocess, augment, and more. It's free for your first 1000 source images.\n",
    "\n",
    "**Looking for a vision model available via API without hassle? Try Roboflow Train.**\n",
    "\n",
    "![Roboflow Wordmark](https://i.imgur.com/dcLNMhV.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZpGQmgIE4ru"
   },
   "source": [
    "# **1. Installing Dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsaSUvj6FX98"
   },
   "source": [
    "# 1.1 Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 8314,
     "status": "error",
     "timestamp": 1664362441212,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     },
     "user_tz": -60
    },
    "id": "uQhvetA-L3Z-",
    "outputId": "b1f842f1-d69a-415e-9286-a856a519c840"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "print(\"Started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-coGSPIFkO6"
   },
   "source": [
    "# 1.2 Installing our dependencies\n",
    "\n",
    "**Note**: While installing dependencies it will prompt to restart runtime, don't worry just restart it and only run the above **1.1 Mounting google drive** cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGQkL0hkMEC2",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441213,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Download YOLOv7 repository and install requirements\n",
    "!git clone https://github.com/augmentedstartups/yolov7.git\n",
    "%cd /yolov7\n",
    "!pip install -r requirements.txt\n",
    "!pip install roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNq_xvfwF65W"
   },
   "source": [
    "# **2. Getting Our Dataset**\n",
    "\n",
    "If you haven't followed the link to dataset given in description, here it is again [Trash Dataset](https://roboflow.com/as-waste)\n",
    "\n",
    "- Follow the link and sign in to your Roboflow account. If you haven't signed up before, first sign up and then sign in\n",
    "- Once you are login, click the **Download this Dataset** tab in the top right corner\n",
    "- A dialogue box will open, select the YOLOv7 format, check the **Show download code** option and press continue.\n",
    "- A download code will appear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzNG2JFDMQ5q",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441213,
     "user_tz": -60,
     "elapsed": 14,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "%cd /yolov7\n",
    "\n",
    "#### ROBOFLOW DATASET DOWNLOAD CODE #####\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"XYyVP6HzbEJypYgwe1aG\")\n",
    "project = rf.workspace(\"nam-nhat\").project(\"trash-dvdrr\")\n",
    "dataset = project.version(5).download(\"yolov7\",location=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYAA-B5fKLW6"
   },
   "source": [
    "# **3. Run YOLOv7 Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieCqTaadHMwe"
   },
   "source": [
    "# 3.1 Getting our pretrained model, you can choose any model from below to fine-tune\n",
    "\n",
    "**Uncomment the model you want to finetune**\n",
    "\n",
    "There are five available model, uncomment the one which you want to train. For this we will be finetuning **yolov7.pt** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnoD0kiwdcim",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441213,
     "user_tz": -60,
     "elapsed": 14,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "%cd /yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Mz-HigZM2xo",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441214,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -P /yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
    "#wget -P /yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.pt\n",
    "# wget -P /yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6.pt\n",
    "# wget -P /yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt\n",
    "# wget -P /yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6.pt\n",
    "# wget -P /yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wm3t5iCBHq8s"
   },
   "source": [
    "# 3.2 Start Training\n",
    "\n",
    "**Note**\n",
    "\n",
    "[To get the full list of training arguments follow the link](https://github.com/WongKinYiu/yolov7/blob/main/train.py)\n",
    "\n",
    "Some important arguments to know\n",
    "- **configuration**: In the main yolov7 folder go to cfg/training folder and select the path of appropriate configuration file. Give the relative path to the file in **--cfg** argument\n",
    "- **data** the path to data folder, it will be automatically catered \n",
    "- **weights** path to pretrained weights given by **--weights** argument\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Note for resuming training from checkpoint** <br>\n",
    "By default, the checkpoints for the epoch are stored in folder, yolov7/runs/train, give the relative path to last epoch checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssOC8XWZN25Y",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441214,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "%cd /yolov7\n",
    "!python train.py --batch 16 --cfg cfg/training/yolov7.yaml --epochs 5 --data {dataset.location}/data.yaml --weights 'yolov7.pt' --device 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdyN-la5LYVm"
   },
   "source": [
    "# **4. Evaluation**\n",
    "\n",
    "- Note the checkpoints from training will be stored by default in runs/train/exp. Take the path of the latest checkpoint\n",
    "\n",
    "We can evaluate the performance of our custom training using the provided evalution script.\n",
    "\n",
    "Note we can adjust the below custom arguments. For details, see [the arguments accepted by detect.py](https://github.com/WongKinYiu/yolov7/blob/main/detect.py#L154)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_iqHZtn94zc"
   },
   "source": [
    "# 4.1 F1 and Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYjFikI48Ngf",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441214,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "display(Image(\"/yolov7/runs/train/exp/F1_curve.png\", width=400, height=400))\n",
    "display(Image(\"/yolov7/runs/train/exp/PR_curve.png\", width=400, height=400))\n",
    "display(Image(\"/yolov7/runs/train/exp/confusion_matrix.png\", width=500, height=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pYQASsRNEKX"
   },
   "source": [
    "# 5.1.1 Run the below cell to evaluate on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4subGsgFOKXq",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441214,
     "user_tz": -60,
     "elapsed": 14,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "!python detect.py --weights /yolov7/runs/train/exp/weights/epoch_054.pt --conf 0.1 --source /yolov7/Trash-5/test/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af7q-cEONIYI"
   },
   "source": [
    "# 5.1.2 Display Inference on Folder of Test Images\n",
    "\n",
    "**Note** From the above output display copy the full path of folder where test images are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79W_rpa1MGMp",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441215,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "#display inference on ALL test images\n",
    "\n",
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "i = 0\n",
    "limit = 10000 # max images to print\n",
    "for imageName in glob.glob('/yolov7/runs/detect/exp2/*.jpg'):\n",
    "    #Assuming JPG\n",
    "    if i < limit:\n",
    "      display(Image(filename=imageName))\n",
    "      print(\"\\n\")\n",
    "    i = i + 1\n",
    "\n",
    "display(Image(\"/yolov7/runs/detect/exp3/52_jpg.rf.c3931652d0d6e62034543e92ec110c0b.jpg\", width=400, height=400))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2EWkmufOChU"
   },
   "source": [
    "# **5.2 Now it's time to Infer on Custom Images**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbp1q19cOrER"
   },
   "source": [
    "## 5.2.1 Helper Code For Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tkfW_FBM4IW",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441215,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/yolov7')\n",
    "\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
    "\n",
    "\n",
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iis5R9nO6kf"
   },
   "source": [
    "# 5.2.2 Configuration Parameters\n",
    "\n",
    "Change the path of both **weights** and **yaml** file\n",
    "\n",
    "**weights** will be in yolov7 main folder -> runs -> train and then select the appropriate weight\n",
    "\n",
    "**yaml** yolov7 main folder -> Trash-5, there you will find yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OR69IOOpNnb4",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441215,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "classes_to_filter = None  #You can give list of classes to filter by name, Be happy you don't have to put class number. ['train','person' ]\n",
    "\n",
    "\n",
    "opt  = {\n",
    "    \n",
    "    \"weights\": \"/yolov7/runs/train/exp/weights/epoch_024.pt\", # Path to weights file default weights are for nano model\n",
    "    \"yaml\"   : \"Trash-5/data.yaml\",\n",
    "    \"img-size\": 640, # default image size\n",
    "    \"conf-thres\": 0.25, # confidence threshold for inference.\n",
    "    \"iou-thres\" : 0.45, # NMS IoU threshold for inference.\n",
    "    \"device\" : '0',  # device to run our model i.e. 0 or 0,1,2,3 or cpu\n",
    "    \"classes\" : classes_to_filter  # list of classes to filter or None\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDiFruFSPsYQ"
   },
   "source": [
    "# **5.3. Inference on Single Image**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnKI_cii2xXY",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441215,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "%cd /yolov7\n",
    "!gdown https://drive.google.com/uc?id=1c96hId8WNsOASKHcAxsQeM4N-N2wuwy9\n",
    "#This does not work in Safari Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReP3h5j-26rw",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441216,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "source_image_path = '/yolov7/trash.jpg'\n",
    "#Change the Path Name to your file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPtGIYnSPnvj",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441216,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Give path of source image.\n",
    "#%cd /yolov7\n",
    "#source_image_path = '/content/trash.png'\n",
    "\n",
    "with torch.no_grad():\n",
    "  weights, imgsz = opt['weights'], opt['img-size']\n",
    "  set_logging()\n",
    "  device = select_device(opt['device'])\n",
    "  half = device.type != 'cpu'\n",
    "  model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "  stride = int(model.stride.max())  # model stride\n",
    "  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "  if half:\n",
    "    model.half()\n",
    "\n",
    "  names = model.module.names if hasattr(model, 'module') else model.names\n",
    "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "  if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "  img0 = cv2.imread(source_image_path)\n",
    "  img = letterbox(img0, imgsz, stride=stride)[0]\n",
    "  img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "  img = np.ascontiguousarray(img)\n",
    "  img = torch.from_numpy(img).to(device)\n",
    "  img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "  img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "  if img.ndimension() == 3:\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "  # Inference\n",
    "  t1 = time_synchronized()\n",
    "  pred = model(img, augment= False)[0]\n",
    "\n",
    "  # Apply NMS\n",
    "  classes = None\n",
    "  if opt['classes']:\n",
    "    classes = []\n",
    "    for class_name in opt['classes']:\n",
    "\n",
    "      classes.append(opt['classes'].index(class_name))\n",
    "\n",
    "\n",
    "  pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= classes, agnostic= False)\n",
    "  t2 = time_synchronized()\n",
    "  for i, det in enumerate(pred):\n",
    "    s = ''\n",
    "    s += '%gx%g ' % img.shape[2:]  # print string\n",
    "    gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n",
    "    if len(det):\n",
    "      det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "      for c in det[:, -1].unique():\n",
    "        n = (det[:, -1] == c).sum()  # detections per class\n",
    "        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "    \n",
    "      for *xyxy, conf, cls in reversed(det):\n",
    "\n",
    "        label = f'{names[int(cls)]} {conf:.2f}'\n",
    "        plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkBIq12rQPsY",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1664362441216,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Jorge Miguel Soares Lopes",
      "userId": "08818112133118791108"
     }
    }
   },
   "outputs": [],
   "source": [
    "#from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "cv2.imshow(img0)\n",
    "print(\"FINISHED\")\n",
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
